[{"title": "glm-4.7-flash", "description": "GLM 4.7 Flash is a 30B A3B MoE model form Z.ai. It supports a context length of 128k tokens and achieves strong performance on coding benchmarks among models of similar scale.", "tags": ["30B", "glm4_moe_lite"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/GLM-4.7-Flash-MLX-6bit", "https://huggingface.co/lmstudio-community/GLM-4.7-Flash-MLX-8bit", "https://huggingface.co/lmstudio-community/GLM-4.7-Flash-GGUF"]}, {"title": "functiongemma-270m", "description": "Lightweight Gemma 3-based model (270M params) trained specifically for function calling. Text-only with a 32k context window, designed to be fine-tuned into your own tool agent while remaining small enough for laptops or edge devices.", "tags": ["270M", "gemma3"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/functiongemma-270m-it-GGUF", "https://huggingface.co/lmstudio-community/functiongemma-270m-it-MLX-bf16", "https://huggingface.co/google/functiongemma-270m-it"]}, {"title": "nemotron-3-nano", "description": "General purpose reasoning and chat model trained from scratch by NVIDIA. Contains 30B total parameters with only 3.5B active at a time for low-latency MoE inference", "tags": ["30B", "nemotron_h_moe"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-8bit", "https://huggingface.co/lmstudio-community/NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-6bit", "https://huggingface.co/lmstudio-community/NVIDIA-Nemotron-3-Nano-30B-A3B-GGUF", "https://huggingface.co/lmstudio-community/NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-5bit", "https://huggingface.co/lmstudio-community/NVIDIA-Nemotron-3-Nano-30B-A3B-MLX-4bit"]}, {"title": "glm-4.6v-flash", "description": "GLM 4.6V Flash is a 9B vision-language model optimized for local deployment and low-latency applications. It supports a context length of 128k tokens and achieves strong performance in visual understanding among models of similar scale.", "tags": ["9B", "glm4v"], "capabilities": "Vision Input\nTrained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/GLM-4.6V-Flash-GGUF", "https://huggingface.co/lmstudio-community/GLM-4.6V-Flash-MLX-6bit", "https://huggingface.co/lmstudio-community/GLM-4.6V-Flash-MLX-4bit", "https://huggingface.co/lmstudio-community/GLM-4.6V-Flash-MLX-8bit"]}, {"title": "devstral-small-2-2512", "description": "Second-generation Devstral Small for agentic coding. Built for tool use to explore codebases, edit multiple files, and power software engineering agents with newly added vision support.", "tags": ["24B", "mistral3"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/lmstudio-community/Devstral-Small-2-24B-Instruct-2512-GGUF", "https://huggingface.co/mlx-community/Devstral-Small-2-24B-Instruct-2512-4bit"]}, {"title": "devstral-2-2512", "description": "Second-generation Devstral for agentic coding with 123B parameters and a 256k context window. Built for tool use to explore codebases, edit multiple files, and power software engineering agents.", "tags": ["123B", "mistral3"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/Devstral-2-123B-Instruct-2512-GGUF"]}, {"title": "rnj-1", "description": "Rnj-1 is an 8B parameter open-weight, dense model trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.", "tags": ["8B", "gemma3"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/rnj-1-instruct-GGUF"]}, {"title": "ministral-3-3b", "description": "The smallest model in the Ministral 3 family, combining a 3.4B language model with a 0.4B vision encoder for efficient edge deployment.", "tags": ["3B", "mistral3"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/lmstudio-community/Ministral-3-3B-Instruct-2512-GGUF"]}, {"title": "ministral-3-3b-reasoning", "description": "The reasoning post-trained version of Ministral 3 3B, combining a 3.4B language model with a 0.4B vision encoder, optimized for complex reasoning tasks.", "tags": ["3B", "mistral3"], "capabilities": "Vision Input\nTrained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Ministral-3-3B-Reasoning-2512-GGUF"]}, {"title": "ministral-3-8b", "description": "An 8B parameter model in the Ministral 3 family, combining a language model with a vision encoder for efficient edge deployment.", "tags": ["8B", "mistral3"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/lmstudio-community/Ministral-3-8B-Instruct-2512-GGUF"]}, {"title": "ministral-3-8b-reasoning", "description": "The reasoning post-trained version of Ministral 3 8B, optimized for complex reasoning tasks.", "tags": ["8B", "mistral3"], "capabilities": "Vision Input\nTrained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Ministral-3-8B-Reasoning-2512-GGUF"]}, {"title": "ministral-3-14b", "description": "A 14B parameter model in the Ministral 3 family, combining a language model with a vision encoder for efficient edge deployment.", "tags": ["14B", "mistral3"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/lmstudio-community/Ministral-3-14B-Instruct-2512-GGUF"]}, {"title": "ministral-3-14b-reasoning", "description": "The reasoning post-trained version of Ministral 3 14B, optimized for complex reasoning tasks.", "tags": ["14B", "mistral3"], "capabilities": "Vision Input\nTrained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Ministral-3-14B-Reasoning-2512-GGUF"]}, {"title": "qwen3-next-80b", "description": "Hybrid attention architecture, high-sparsity Mixture-of-Experts 80B model (active 3B). Currently supported for Mac only with MLX.", "tags": ["80B", "qwen3_next"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-Next-80B-A3B-Instruct-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-Next-80B-A3B-Instruct-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-Next-80B-A3B-Instruct-MLX-6bit", "https://huggingface.co/lmstudio-community/Qwen3-Next-80B-A3B-Instruct-MLX-5bit", "https://huggingface.co/lmstudio-community/Qwen3-Next-80B-A3B-Instruct-GGUF"]}, {"title": "olmo-3-7b", "description": "Olmo is a series of Open language models designed to enable the science of language models.", "tags": ["7B", "olmo3"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/Olmo-3-7B-Instruct-MLX-8bit", "https://huggingface.co/lmstudio-community/Olmo-3-7B-Instruct-MLX-4bit", "https://huggingface.co/lmstudio-community/Olmo-3-7B-Instruct-GGUF", "https://huggingface.co/lmstudio-community/Olmo-3-7B-Instruct-MLX-6bit"]}, {"title": "olmo-3-7b-think", "description": "", "tags": ["7B", "olmo3"], "capabilities": "Reasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Olmo-3-7B-Think-MLX-8bit", "https://huggingface.co/lmstudio-community/Olmo-3-7B-Think-GGUF", "https://huggingface.co/lmstudio-community/Olmo-3-7B-Think-MLX-6bit", "https://huggingface.co/lmstudio-community/Olmo-3-7B-Think-MLX-4bit"]}, {"title": "olmo-3-32b-think", "description": "AllenAI's flagship post-trained reasoning model built on Olmo 3-Base", "tags": ["32B", "olmo3"], "capabilities": "Reasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Olmo-3-32B-Think-MLX-6bit", "https://huggingface.co/lmstudio-community/Olmo-3-32B-Think-MLX-4bit", "https://huggingface.co/lmstudio-community/Olmo-3-32B-Think-GGUF", "https://huggingface.co/lmstudio-community/Olmo-3-32B-Think-MLX-8bit"]}, {"title": "olmocr-2-7b", "description": "The olmOCR 2 model is a Vision Language Model (VLM) from Allen AI.", "tags": ["7B", "qwen2vl"], "capabilities": "Vision Input", "links": ["https://huggingface.co/lmstudio-community/olmOCR-2-7B-1025-GGUF", "https://huggingface.co/datasets/allenai/olmOCR-mix-1025"]}, {"title": "qwen2.5-vl-7b", "description": "a 7B Vision Language Model (VLM) from the Qwen2.5 family", "tags": ["7B", "qwen2vl"], "capabilities": "Vision Input", "links": ["https://huggingface.co/lmstudio-community/Qwen2.5-VL-7B-Instruct-GGUF"]}, {"title": "minimax-m2", "description": "MiniMax M2 is a 230B MoE (10 active) LLM, built for coding and agentic workflows.", "tags": ["230B", "minimax-m2"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/MiniMax-M2-MLX-8bit", "https://huggingface.co/lmstudio-community/MiniMax-M2-MLX-4bit", "https://huggingface.co/lmstudio-community/MiniMax-M2-MLX-6bit", "https://huggingface.co/lmstudio-community/MiniMax-M2-GGUF"]}, {"title": "gpt-oss-safeguard-20b", "description": "", "tags": ["20B", "gpt-oss"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/gpt-oss-safeguard-20b-MLX-MXFP4", "https://huggingface.co/lmstudio-community/gpt-oss-safeguard-20b-GGUF"]}, {"title": "gpt-oss-safeguard-120b", "description": "", "tags": ["120B", "gpt-oss"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/gpt-oss-safeguard-120b-GGUF", "https://huggingface.co/lmstudio-community/gpt-oss-safeguard-120b-MLX-MXFP4"]}, {"title": "qwen3-vl-2b", "description": "", "tags": ["2B", "qwen3_vl"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-VL-2B-Instruct-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-VL-2B-Instruct-MLX-8bit"]}, {"title": "qwen3-vl-4b", "description": "The 4B version of Qwen's latest vision-language model. Includes comprehensive upgrades to visual perception, spatial reasoning, and image understanding.", "tags": ["4B", "qwen3_vl"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-VL-4B-Instruct-MLX-5bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-4B-Instruct-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-VL-4B-Instruct-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-4B-Instruct-MLX-6bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-4B-Instruct-MLX-4bit"]}, {"title": "qwen3-vl-8b", "description": "The 8B version of Qwen's latest vision-language model. Includes comprehensive upgrades to visual perception, spatial reasoning, and image understanding.", "tags": ["8B", "qwen3_vl"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-VL-8B-Instruct-MLX-5bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-8B-Instruct-MLX-6bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-8B-Instruct-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-8B-Instruct-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-8B-Instruct-GGUF"]}, {"title": "qwen3-vl-30b", "description": "The latest generation vision-language MoE model in the Qwen series with comprehensive upgrades to visual perception, spatial reasoning, and image understanding.", "tags": ["30B", "qwen3_vl_moe"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-VL-30B-A3B-Instruct-MLX-6bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-30B-A3B-Instruct-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-30B-A3B-Instruct-MLX-5bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-30B-A3B-Instruct-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-30B-A3B-Instruct-GGUF"]}, {"title": "qwen3-vl-32b", "description": "", "tags": ["32B", "qwen3_vl"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-VL-32B-Instruct-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-32B-Instruct-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-VL-32B-Instruct-MLX-6bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-32B-Instruct-MLX-5bit", "https://huggingface.co/lmstudio-community/Qwen3-VL-32B-Instruct-MLX-8bit"]}, {"title": "granite-4-h-micro", "description": "A hybrid dense model trained for tool use from IBM.", "tags": ["3B", "granitehybrid"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/granite-4.0-h-micro-GGUF"]}, {"title": "granite-4-micro", "description": "A dense transformer model trained for tool use from IBM.", "tags": ["3B", "granite"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/granite-4.0-micro-GGUF"]}, {"title": "granite-4-h-tiny", "description": "A hybrid MoE model trained for tool use from IBM.", "tags": ["7B", "granitehybrid"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/granite-4.0-h-tiny-GGUF"]}, {"title": "granite-4-h-small", "description": "A hybrid MoE model trained for tool use from IBM.", "tags": ["32B", "granitehybrid"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/granite-4.0-h-small-GGUF"]}, {"title": "seed-oss-36b", "description": "Advanced reasoning model from ByteDance with flexible \"thinking budget\" control and ability to reflect on the length of its own reasoning", "tags": ["36B", "seed_oss"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Seed-OSS-36B-Instruct-GGUF", "https://huggingface.co/lmstudio-community/Seed-OSS-36B-Instruct-MLX-4bit", "https://huggingface.co/lmstudio-community/Seed-OSS-36B-Instruct-MLX-6bit", "https://huggingface.co/lmstudio-community/Seed-OSS-36B-Instruct-MLX-8bit", "https://huggingface.co/lmstudio-community/Seed-OSS-36B-Instruct-MLX-5bit"]}, {"title": "qwen3-4b-2507", "description": "Updated version of Qwen3 4B non-thinking mode featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage.", "tags": ["4B", "qwen3"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-MLX-5bit", "https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-MLX-6bit", "https://huggingface.co/lmstudio-community/Qwen3-4b-Instruct-2507-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF"]}, {"title": "qwen3-4b-thinking-2507", "description": "Updated thinking version of Qwen3 4B featuring continued scaling of thinking capability, improving both the quality and depth of reasoning", "tags": ["4B", "qwen3"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-MLX-6bit"]}, {"title": "qwen3-30b-a3b-2507", "description": "Updated version of Qwen3-30B-A3B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage.", "tags": ["30B", "qwen3moe"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-MLX-6bit", "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Instruct-2507-GGUF"]}, {"title": "qwen3-30b-a3b-thinking-2507", "description": "Always-thinking version of Qwen3-30B-A3B featuring significant improvements on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise", "tags": ["30B", "qwen3moe"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Thinking-2507-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Thinking-2507-MLX-6bit", "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Thinking-2507-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-Thinking-2507-MLX-4bit"]}, {"title": "qwen3-235b-a22b-2507", "description": "Updated version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage.", "tags": ["235B", "qwen3moe"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-235B-A22B-Instruct-2507-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-235B-A22B-Instruct-2507-MLX-6bit", "https://huggingface.co/lmstudio-community/Qwen3-235B-A22B-Instruct-2507-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-235B-A22B-Instruct-2507-MLX-4bit"]}, {"title": "qwen3-235b-a22b-thinking-2507", "description": "Enhanced version of Qwen3-235B-A22B featuring significant improvements in thinking and reasoning capabilities.", "tags": ["235B", "qwen3moe"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Qwen3-235B-A22B-Thinking-2507-MLX-6bit", "https://huggingface.co/lmstudio-community/Qwen3-235B-A22B-Thinking-2507-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-235B-A22B-Thinking-2507-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-235B-A22B-Thinking-2507-GGUF"]}, {"title": "gpt-oss-20b", "description": "The 20B variant of OpenAI's open source model. Apache 2.0 licensed.", "tags": ["20B", "gpt-oss"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/gpt-oss-20b-GGUF", "https://huggingface.co/mlx-community/gpt-oss-20b-MXFP4-Q8"]}, {"title": "gpt-oss-120b", "description": "The 120B variant of OpenAI's open source model. Apache 2.0 licensed.", "tags": ["120B", "gpt-oss"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/gpt-oss-120b-mlx-8bit", "https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF"]}, {"title": "qwen3-coder-30b", "description": "A powerful 30B MoE coding model from Alibaba Qwen, joining its larger 480B counterpart", "tags": ["30B", "qwen3moe"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-5bit", "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-6bit"]}, {"title": "qwen3-coder-480b", "description": "Qwen's most powerful code model, featuring 480B total parameters with 35B activated through Mixture of Experts (MoE) architecture.", "tags": ["480B", "qwen3_moe"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/Qwen3-Coder-480B-A35B-Instruct-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-Coder-480B-A35B-Instruct-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-Coder-480B-A35B-Instruct-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-Coder-480B-A35B-Instruct-MLX-6bit"]}, {"title": "ernie-4.5-21b-a3b", "description": "Medium-size mixture-of-experts model from Baidu's new Ernie 4.5 line of foundation models", "tags": ["21B", "ernie4_5"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/ERNIE-4.5-21B-A3B-MLX-6bit", "https://huggingface.co/lmstudio-community/ERNIE-4.5-21B-A3B-MLX-4bit", "https://huggingface.co/lmstudio-community/ERNIE-4.5-21B-A3B-MLX-8bit", "https://huggingface.co/lmstudio-community/ERNIE-4.5-21B-A3B-PT-GGUF"]}, {"title": "lfm2-350m", "description": "Hybrid architecture model intended for local use, by Liquid AI", "tags": ["350M", "lfm2"], "capabilities": "", "links": ["https://huggingface.co/LiquidAI/LFM2-350M-GGUF", "https://huggingface.co/lmstudio-community/LFM2-350M-MLX-bf16", "https://huggingface.co/lmstudio-community/LFM2-350M-MLX-8bit"]}, {"title": "lfm2-700m", "description": "Hybrid architecture model intended for local use, by Liquid AI", "tags": ["700M", "lfm2"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/LFM2-700M-MLX-8bit", "https://huggingface.co/lmstudio-community/LFM2-700M-MLX-bf16", "https://huggingface.co/LiquidAI/LFM2-700M-GGUF"]}, {"title": "lfm2-1.2b", "description": "Hybrid architecture model intended for local use, by Liquid AI", "tags": ["1.2B", "lfm2"], "capabilities": "", "links": ["https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF", "https://huggingface.co/lmstudio-community/LFM2-1.2B-MLX-8bit", "https://huggingface.co/lmstudio-community/LFM2-1.2B-MLX-bf16"]}, {"title": "LFM2-350M", "description": "Hybrid architecture model intended for local use, by Liquid AI", "tags": ["350M", "lfm2"], "capabilities": "", "links": ["https://huggingface.co/LiquidAI/LFM2-350M-GGUF", "https://huggingface.co/lmstudio-community/LFM2-350M-MLX-bf16", "https://huggingface.co/lmstudio-community/LFM2-350M-MLX-8bit"]}, {"title": "LFM2-700M", "description": "Hybrid architecture model intended for local use, by Liquid AI", "tags": ["700M", "lfm2"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/LFM2-700M-MLX-8bit", "https://huggingface.co/lmstudio-community/LFM2-700M-MLX-bf16", "https://huggingface.co/LiquidAI/LFM2-700M-GGUF"]}, {"title": "LFM2-1.2B", "description": "Hybrid architecture model intended for local use, by Liquid AI", "tags": ["1.2B", "lfm2"], "capabilities": "", "links": ["https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF", "https://huggingface.co/lmstudio-community/LFM2-1.2B-MLX-8bit", "https://huggingface.co/lmstudio-community/LFM2-1.2B-MLX-bf16"]}, {"title": "devstral-small-2507", "description": "Devstral excels at using tools to explore codebases and editing multiple files to power software engineering agents.", "tags": ["24B", "mistral"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/Devstral-Small-2507-MLX-6bit", "https://huggingface.co/lmstudio-community/Devstral-Small-2507-MLX-4bit", "https://huggingface.co/lmstudio-community/Devstral-Small-2507-GGUF", "https://huggingface.co/lmstudio-community/Devstral-Small-2507-MLX-8bit", "https://huggingface.co/lmstudio-community/Devstral-Small-2507-MLX-bf16"]}, {"title": "devstral-small-2505", "description": "Devstral by MistralAI is based on Mistral Small 3.1. Debuts as the #1 open source model on SWE-bench.", "tags": ["23.6B", "mistral"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/Devstral-Small-2505-GGUF", "https://huggingface.co/lmstudio-community/devstral-small-2505-MLX-4bit"]}, {"title": "devstral-small-2505", "description": "Devstral by MistralAI is based on Mistral Small 3.1. Debuts as the #1 open source model on SWE-bench.", "tags": ["23.6B", "mistral"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/Devstral-Small-2505-GGUF", "https://huggingface.co/lmstudio-community/devstral-small-2505-MLX-4bit"]}, {"title": "gemma-3n-e2b", "description": "", "tags": ["4.5B", "gemma3n"], "capabilities": "Vision supported (mixed)", "links": ["https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-MLX-8bit", "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-text-GGUF", "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-MLX-4bit", "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-MLX-bf16", "https://huggingface.co/lmstudio-community/gemma-3n-E2B-it-MLX-6bit"]}, {"title": "gemma-3n-e4b", "description": "Gemma 3n is a multimodal generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets.", "tags": ["6.9B", "gemma3n"], "capabilities": "Vision supported (mixed)", "links": ["https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-MLX-bf16", "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-text-GGUF", "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-MLX-4bit", "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-MLX-8bit", "https://huggingface.co/lmstudio-community/gemma-3n-E4B-it-MLX-6bit"]}, {"title": "mistral-small-3.2", "description": "Update to Mistral Small 3.1 with better instruction following, fewer infinite generation issues, and an improved tone.", "tags": ["24B", "mistral"], "capabilities": "Vision Input", "links": ["https://huggingface.co/lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-MLX-8bit", "https://huggingface.co/lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-MLX-6bit", "https://huggingface.co/lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-GGUF", "https://huggingface.co/lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-MLX-4bit"]}, {"title": "magistral-small-2509", "description": "Reasoning model that supports image input and tools calling. By MistralAI.", "tags": ["24B", "mistral"], "capabilities": "Vision Input\nTrained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Magistral-Small-2509-MLX-5bit", "https://huggingface.co/lmstudio-community/Magistral-Small-2509-GGUF", "https://huggingface.co/lmstudio-community/Magistral-Small-2509-MLX-8bit", "https://huggingface.co/lmstudio-community/Magistral-Small-2509-MLX-6bit", "https://huggingface.co/lmstudio-community/Magistral-Small-2509-MLX-4bit"]}, {"title": "magistral-small", "description": "MistralAI's first reasoning model, based on Mistral Small 3.1", "tags": ["23.6B", "mistral"], "capabilities": "Reasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Magistral-Small-2506-MLX-4bit", "https://huggingface.co/lmstudio-community/Magistral-Small-2506-GGUF", "https://huggingface.co/lmstudio-community/magistral-small-2506-mlx-bf16"]}, {"title": "mistral-nemo-instruct-2407", "description": "A slightly larger 12B parameter model from Mistral AI, NeMo offers a long 128k token context length, advanced world knowledge, and function calling for developers.", "tags": ["12B", "mistral"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/Mistral-Nemo-Instruct-2407-GGUF"]}, {"title": "qwen2.5-vl-3b", "description": "", "tags": ["3B", "qwen2vl"], "capabilities": "Vision Input", "links": ["https://huggingface.co/lmstudio-community/Qwen2.5-VL-3B-Instruct-GGUF"]}, {"title": "qwen2.5-vl-7b", "description": "a 7B Vision Language Model (VLM) from the Qwen2.5 family", "tags": ["7B", "qwen2vl"], "capabilities": "Vision Input", "links": ["https://huggingface.co/lmstudio-community/Qwen2.5-VL-7B-Instruct-GGUF"]}, {"title": "qwen2.5-vl-32b", "description": "", "tags": ["32B", "qwen2vl"], "capabilities": "Vision Input", "links": ["https://huggingface.co/lmstudio-community/Qwen2.5-VL-32B-Instruct-GGUF"]}, {"title": "qwen2.5-vl-72b", "description": "", "tags": ["72B", "qwen2vl"], "capabilities": "Vision Input", "links": ["https://huggingface.co/lmstudio-community/Qwen2.5-VL-72B-Instruct-GGUF"]}, {"title": "gemma-3-4b", "description": "State-of-the-art image + text input models from Google, built from the same research and tech used to create the Gemini models", "tags": ["4B", "gemma3"], "capabilities": "Vision Input", "links": ["https://huggingface.co/lmstudio-community/gemma-3-4B-it-QAT-GGUF", "https://huggingface.co/mlx-community/gemma-3-4b-it-qat-4bit", "https://huggingface.co/lmstudio-community/gemma-3-4b-it-GGUF"]}, {"title": "gemma-3-12b", "description": "State-of-the-art image + text input models from Google, built from the same research and tech used to create the Gemini models", "tags": ["12B", "gemma3"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/lmstudio-community/gemma-3-12B-it-QAT-GGUF", "https://huggingface.co/lmstudio-community/gemma-3-12b-it-GGUF", "https://huggingface.co/mlx-community/gemma-3-12b-it-qat-4bit"]}, {"title": "gemma-3-27b", "description": "State-of-the-art image + text input models from Google, built from the same research and tech used to create the Gemini models", "tags": ["27B", "gemma3"], "capabilities": "Vision Input\nTrained for tool use", "links": ["https://huggingface.co/mlx-community/gemma-3-27b-it-qat-4bit", "https://huggingface.co/lmstudio-community/gemma-3-27B-it-qat-GGUF"]}, {"title": "gemma-3-270m", "description": "Smallest member of the Gemma 3 family. Supports a context length of 32k tokens.", "tags": ["270M", "gemma3"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/gemma-3-270m-it-qat-GGUF"]}, {"title": "gemma-3-1b", "description": "Tiny text-only variant of Gemma 3: Google's latest open-weight model family", "tags": ["1B", "gemma3"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/gemma-3-1B-it-QAT-GGUF", "https://huggingface.co/mlx-community/gemma-3-1b-it-qat-4bit"]}, {"title": "phi-4-mini-reasoning", "description": "Lightweight open model from the Phi-4 family", "tags": ["3.8B", "phi-4"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Phi-4-mini-reasoning-GGUF", "https://huggingface.co/lmstudio-community/Phi-4-mini-reasoning-MLX-4bit"]}, {"title": "phi-4-reasoning-plus", "description": "Advanced open-weight reasoning model, finetuned from Phi-4 with additional reinforcement learning for higher accuracy", "tags": ["14.7B", "phi-4"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Phi-4-reasoning-plus-GGUF", "https://huggingface.co/lmstudio-community/Phi-4-reasoning-plus-MLX-4bit"]}, {"title": "phi-4-reasoning", "description": "State-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning", "tags": ["14.7B", "phi-4"], "capabilities": "Trained for tool use", "links": ["https://huggingface.co/lmstudio-community/Phi-4-reasoning-GGUF", "https://huggingface.co/lmstudio-community/Phi-4-reasoning-MLX-4bit"]}, {"title": "phi-4", "description": "The latest in the Phi model series: suitable for chats with a context of up to 16K tokens", "tags": ["14B", "phi"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/phi-4-GGUF"]}, {"title": "phi-4-mini", "description": "", "tags": ["3B", "phi"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/Phi-4-mini-instruct-GGUF"]}, {"title": "codestral-22b-v0.1", "description": "Mistral AI's latest coding model, Codestral can handle both instructions and code completions with ease in over 80 programming languages.", "tags": ["22B", "mistral"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/Codestral-22B-v0.1-GGUF"]}, {"title": "mistral-7b-instruct-v0.3", "description": "One of the most popular open-source LLMs, Mistral's 7B Instruct model's balance of speed, size, and performance makes it a great general-purpose daily driver.", "tags": ["7B", "mistral"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF"]}, {"title": "qwen3-4b", "description": "The 4B parameter version of the Qwen3 model family.", "tags": ["4B", "qwen3"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Qwen3-4B-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-4B-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-4B-MLX-8bit"]}, {"title": "qwen3-8b", "description": "The 8B parameter version of the Qwen3 model family.", "tags": ["8B", "qwen3"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Qwen3-8B-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-8B-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-8B-MLX-4bit"]}, {"title": "qwen3-14b", "description": "The 14B parameter version of the Qwen3 model family.", "tags": ["14B", "qwen3"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Qwen3-14B-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-14B-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-14B-MLX-8bit"]}, {"title": "qwen3-30b-a3b", "description": "The 30B parameter (MoE) version of the Qwen3 model family.", "tags": ["30B", "qwen3moe"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-4bit", "https://huggingface.co/lmstudio-community/Qwen3-30B-A3B-MLX-8bit"]}, {"title": "qwen3-32b", "description": "The 32B parameter version of the Qwen3 model family.", "tags": ["32B", "qwen3"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/Qwen3-32B-MLX-8bit", "https://huggingface.co/lmstudio-community/Qwen3-32B-GGUF", "https://huggingface.co/lmstudio-community/Qwen3-32B-MLX-4bit"]}, {"title": "qwen3-235b-a22b", "description": "The 235B parameter (MoE) version of the Qwen3 model family.", "tags": ["235B", "qwen3moe"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/mlx-community/Qwen3-235B-A22B-4bit", "https://huggingface.co/mlx-community/Qwen3-235B-A22B-8bit", "https://huggingface.co/lmstudio-community/Qwen3-235B-A22B-GGUF"]}, {"title": "deepseek-r1-0528-qwen3-8b", "description": "Distilled version of the DeepSeek-R1-0528 model, created by continuing the post-training process on the Qwen3 8B Base model using Chain-of-Thought (CoT) from DeepSeek-R1-0528.", "tags": ["8B", "qwen3"], "capabilities": "Trained for tool use\nReasoning\nSupports reasoning", "links": ["https://huggingface.co/lmstudio-community/DeepSeek-R1-0528-Qwen3-8B-MLX-4bit", "https://huggingface.co/lmstudio-community/DeepSeek-R1-0528-Qwen3-8B-GGUF", "https://huggingface.co/lmstudio-community/DeepSeek-R1-0528-Qwen3-8B-MLX-8bit"]}, {"title": "deepseek-r1-distill-qwen-7b", "description": "DeepSeek R1 Distill Qwen 7B by deepseek-ai", "tags": ["7B", "qwen2"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF"]}, {"title": "deepseek-r1-distill-llama-8b", "description": "DeepSeek R1 Distill Llama 8B by deepseek-ai", "tags": ["8B", "llama"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/DeepSeek-R1-Distill-Llama-8B-GGUF"]}, {"title": "deepseek-r1-distill-qwen-14b", "description": "", "tags": ["14B", "qwen2"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF"]}, {"title": "deepseek-r1-distill-qwen-32b", "description": "", "tags": ["32B", "qwen2"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/DeepSeek-R1-Distill-Qwen-32B-GGUF"]}, {"title": "deepseek-r1-distill-llama-70b", "description": "", "tags": ["70B", "llama"], "capabilities": "", "links": ["https://huggingface.co/lmstudio-community/DeepSeek-R1-Distill-Llama-70B-GGUF"]}]